#!/usr/bin/env groovy 

import durbin.weka.*
import durbin.weka.InstanceUtils as IU
import durbin.util.*
import durbin.digma.*

import weka.core.*
import weka.core.converters.ArffSaver
import weka.filters.*
import weka.filters.unsupervised.attribute.Remove
import weka.filters.unsupervised.instance.RemoveWithValues

import weka.classifiers.*
import weka.classifiers.evaluation.*
import weka.classifiers.Evaluation
import weka.classifiers.functions.supportVector.PolyKernel

import weka.classifiers.meta.*
import weka.attributeSelection.*
import weka.attributeSelection.InfoGainAttributeEval

import weka.classifiers.meta.GridSearch;

// Alias sugar. 
err = System.err

// Add some sugar methods to weka.Instances with Groovy metaclass. 
WekaAdditions.enable()     

parser = new Parser(description: '''
 paradigmClassifierEval takes a data file in attributes (rows) x samples (cols) format, 
 and a clinical file, also in attributes (rows) x samples (cols) format, and evaluate 
 the given Weka classifier predicting a specified clinical attribute.  Classification is 
 performed on the samples that occur in both the data and clinical file (intersection). 
 stdout is a list of summary lines in CSV format. Instances with missing class values are removed.   
 Attributes that do not vary at all are removed.  Instances with negative class values are 
 optionally removed.  
 
 Script written to process output of PARADIGM.  Written by: James Durbin (kdurbin@ucsc.edu)
 
 Example:
 
 paradigmClassifierBulkEval -d chin_naderi_pathwayvalues.txt -i chinNaderi_clinical.txt -c Survival -D 9,11 > summary.txt 
 
''');

parser.with{
    
  required 'd','data', [description: 'Data file in attribute (row) by samples (col) format.']
  required 'i','clinical', [description: 'Clinical file in attribute (row) by samples (col) format.']
  optional 'c','config',[description: 'Configuration file. May include embedded experiment list.']  

  optional 'l','experimentList',[description: 'Flat experiment list file.']  
  optional 'r','experimentRange',[description: 'Range of experiments to run. e.g. -r 54,67']
  flag 'h','help',[default:false,description: 'Print script help.']
}

// Validate attributes that one of a set are required...
parser.validate{parameters->
  count = 0
  if (parameters['c']) count++
  if (parameters['f']) count++
  if (count != 1) throw new Exception("Must specify exactly one of: -c, or -F.")
}

def options
try{
  options = parser.parse(args)
}catch(Exception e){
  System.err << parser.usage
  System.exit(1)
}

/***********************************************************************************
*                                      SETUP
*/ 
def cfg = new ConfigSlurper().parse(new File(options.config).toURL())
params = cfg.params // sugar. 

experiments = []
if (options.experimentList){
  new File(options.experimentList).eachLine{line->
    if (!line.startsWith('//')){      
      experiments << new ExperimentSpec(line);
    }
  }
}else{
  // TBD
  // Takes a compact description of all of the classifiers and expands it to a short list. 
  //  classifiers = ClassifierSpecExpander.expand(params)
}
// Set the 
experimentStart = 0
experimentEnd = experiments.size()-1
if (options.experimentRange){
 experimentStart = options.experimentRange.split(",")[0] as int
 experimentEnd = options.experimentRange.split(",")[1] as int
 
 err.println "eStart:"+experimentStart+" eEnd: "+experimentEnd+" size="+experiments.size()
 
}


/***********************************************************************************
*                               MAIN PROCESSING 
*/ 

// Write out heading for the stats output... there are a lot, so heading needed. 
summaryHeading = DigmaPipeline.getFormattedSummaryHeading()

// don't want to print it because we'll be concatenating output from lots of jobs, heading would 
// appear hundreds of times in file. 
//println "${summaryHeading},classifier, attributeEval,attributeSearch,numAttributes,classAttribute"

// Read data and clinical files as a set of Weka Instances
Instances data = DigmaPipeline.readFromTable(options.data)
Instances clinicalData = DigmaPipeline.readFromTable(options.clinical)

// For each experiment...
experiments[experimentStart..experimentEnd].each{experiment-> 
  
  pipeline = new DigmaPipeline(experiment.classAttribute)
  
  // Remove all clinical attributes except for the current class...
  selectedAttribute = []
  selectedAttribute.add(experiment.classAttribute)
  singleClinicalAttribute = pipeline.removeAttributesNotSelected(clinicalData,selectedAttribute)  

  // Merge data and clinical files (i.e. instances contained in both, omitting rest)
  merged = IU.mergeNamedInstances(data,singleClinicalAttribute)

  // Remove any attributes that don't vary at all..
  merged = pipeline.removeUselessAttributes(merged)
    
  // For many class-attributes, negative values are invalid.  
  if (!params.allowNegativeClass){    
    merged = pipeline.removeInstancesWithNegativeClassValues(merged)
  }

  // Remove any instances whose class value is missingValue(). 
  // KJD why not:  newData.deleteWithMissingClass();
  merged = pipeline.removeInstancesWithMissingClassValues(merged) 

  // Discretize class 
  if (params.useMedianDiscretization){
    //merged = pipeline.classToMedian(merged,"low","high")        
    err.println "Not yet implemented:  median discretization."
    System.exit(1)
  }else if (params.useUpperLowerQuartile){
    merged = pipeline.classToNominalTopBottomQuartile(merged,"low","high")        
  }else if (params.discretizeCutoffs){
    lowerBound = params.discretizeCutoffs.low
    upperBound = params.discretizeCutoffs.high
    merged = pipeline.classToNominalFromCutoffs(merged,lowerBound,upperBound,"low","high")
  }

  // If an arffFile output name was given, just save this processed file for 
  // use in Weka GUI and exit. 
  if (params.arffFile){
    ArffSaver saver = new ArffSaver();
    saver.setInstances(merged);
    err.print "\nSaving ${params.arffFile} for manual processing..."
    saver.setFile(new File(params.arffFile))
    saver.writeBatch();  
    err.println "done. Exiting."
    System.exit(0); // Exit happily. 
  }
      
  // KJD... should do this with filteredClassifier...
  merged = pipeline.removeID(merged)        
      
  // Check to see if we can do this experiment at all...
  if (merged.numInstances() < params.cvFolds) {
    err.println "Experiment skipped.  ${merged.numInstances} instances < ${params.cvFolds} cross-validation folds."
    return; // Can't do the experiment. 
  } 
                
  def asClassifier
  if (params.noAttributeSelection) asClassifier = classifierFromSpec(classifierSpec)
  else{
    // Wrap classifier in an AttributeSelectedClassifier. 
    asClassifier = new AttributeSelectedClassifier();
    search = new Ranker();
    search.setNumToSelect(experiment.numAttributes);
    asClassifier.setClassifier(experiment.classifier);
    asClassifier.setEvaluator(experiment.attributeEval);
    asClassifier.setSearch(experiment.attributeSearch);                
  }  
    
  // KJD TODO:  Need to Skip cases where the class doesn't vary!!!
   
  // Perform cross-validation of the model..
  err.print "Instances: ${merged.numInstances()} Attributes: ${experiment.numAttributes}. Running cross validation (${params.cvFolds} folds)..."
  eval = new Evaluation(merged)
  eval.crossValidateModel(asClassifier,merged,params.cvFolds,new Random(params.cvSeed))
  err.println "done. ${experiment.classifierStr} #att: ${experiment.numAttributes} AUC=${eval.weightedAreaUnderROC()}"

  // Append a summary line to a file. 
  summaryLine = pipeline.getFormattedEvaluationSummary(merged.numInstances(),eval)
  println "$summaryLine,${experiment.classifierStr},${experiment.attrEvalStr},${experiment.attrSearchStr},${experiment.numAttributes},${experiment.classAttribute}"

  if (params.rocDir){
    // Save the ROC values for later plotting. 
    ThresholdCurve tc = new ThresholdCurve();
    Instances curve = tc.getCurve(eval.predictions());
    
    ArffSaver saver = new ArffSaver();
    saver.setInstances(curve);
    curveFile = new File("${params.rocDir}${className}.${numFeatures}.arff".toString())
    saver.setFile(curveFile);
    saver.writeBatch();        
  }
} // classifiers


// Example: 
// scripts/paradigmClassifierEval -d dataDec9/breast/chin_naderi_pathwayvalues.txt -i dataDec9/breast/chinNaderi_clinical.txt -c Survival -D 9,11 > test/summary.txt

