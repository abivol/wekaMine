#!/usr/bin/env groovy 

import durbin.weka.*
import durbin.weka.InstanceUtils as IU
import durbin.util.*
//import durbin.paradigm.*

import weka.core.*
import weka.core.converters.ArffSaver
import weka.filters.*
import weka.filters.unsupervised.attribute.Remove
import weka.filters.unsupervised.instance.RemoveWithValues

import weka.filters.unsupervised.attribute.RemoveType
import weka.classifiers.*
import weka.classifiers.evaluation.*
import weka.classifiers.Evaluation
import weka.classifiers.functions.supportVector.PolyKernel

import weka.classifiers.meta.*
import weka.attributeSelection.*
import weka.attributeSelection.InfoGainAttributeEval

import weka.classifiers.meta.GridSearch;

// Alias sugar. 
err = System.err

// Add some sugar methods to weka.Instances with Groovy metaclass. 
WekaAdditions.enable()     

parser = new Parser(description: '''
 wekaBulkEval takes a tab delimited data file in attributes (rows) x samples (cols) format, 
 and a clinical file, also in attributes (rows) x samples (cols) format, and evaluates 
 the set of experiments specified in an experimental configuration file.  Classification is 
 performed on the samples that occur in both the data and clinical file (intersection). 
 A list of summary lines in CSV format will be APPENDED to the end of the output file.  Since this will
 often be run in a cluster setting with many jobs appending to the same file, the heading is not normally 
 written out.  Run this script with the -H option to just output heading to stdout.  
 
 Instances with missing class values are removed.   Attributes that do not vary at all 
 are removed.  Instances with negative class values are optionally removed.  
 
 The actual experiments and experimental options are provided by the config file (-c). 
 Documentation for this config file can be found here:  
 
 https://cancer2.cse.ucsc.edu/mediawiki/index.php/WekaClassifierConfig
 
 Note that the config file can specify a number of attributes to use in attribute 
 selection, or -1 to auto-select (varies from algorithm to algorithm). The config 
 file option actually determines how many attributes the classifier sees.  The 
 -m/maxFeaturesOut option merely determines the maximum number of selected attributes, 
 and their scores, to save in the output and does not affect what the classifier 
 will see. 
 
  
Written by: James Durbin (kdurbin@ucsc.edu)
 
 Example:
 
 wekaBulkEval  \\
 -d data/paradigm_results.txt -i data/collisonclinical.small.tab \\
 -c exp/cfgExample2.txt -r 0,19 -o results/expout.csv
 
''');

parser.with{
    
  required 'd','data', [description: 'Data file in attribute (row) by samples (col) format.']
  required 'i','clinical', [description: 'Clinical file in attribute (row) by samples (col) format.']
  required 'o','output',[description: 'File where output should go.']
  optional 'c','config',[description: 'Configuration file']  

  optional 'm','maxFeaturesOut',[default: 0,description: "Maximum number of ranked features to output from cross-validation or full-set if classifier is 'None'"]
  optional 'R','raRoot',[description: 'Root name of .ra file output']
  optional 'r','experimentRange',[description: 'Range of experiments to run (e.g. -r 54,67, mainly for cluster splits)']
  optional 'l','experimentList',[description: 'Flat experiment list file. OVERRIDES experiments in config. Will probably be deprecated.']  
  flag 'H','writeHeading',[default:false,description: 'Write out heading alone to output file.']
  flag 'h','help',[default:false,description: 'Print script help.']
}

// Validate attributes that one of a set are required...
parser.validate{parameters->
  count = 0
  if (parameters['c']) count++
  if (parameters['f']) count++
  if (count != 1) throw new Exception("Must specify exactly one of: -c, or -F.")
}

def options
try{
  options = parser.parse(args)
}catch(Exception e){
  System.err << parser.usage
  System.exit(1)
}

out = new File(options.output)


/***********************************************************************************
*                                      SETUP
*/ 
def slurper = new WekaExplorerConfigSlurper()
def cfg = slurper.parse(new File(options.config).toURL())
params = cfg.params // cfg includes all sections.  make alias for section we want. 

//params.cvSeed = 521;

experiments = []
if (options.experimentList){
  new File(options.experimentList).eachLine{line->
    if (!line.startsWith('//')){      
      experiments << new ExperimentSpec(line);
    }
  }
}else{
  slurper.getExpansion('experiments').each{experiment->
    experiments << new ExperimentSpec(experiment);
  }
}

// Set the 
experimentStart = 0
experimentEnd = experiments.size()-1
if (options.experimentRange){
 experimentStart = options.experimentRange.split(",")[0] as int
 experimentEnd = options.experimentRange.split(",")[1] as int 
 err.println "eStart:"+experimentStart+" eEnd: "+experimentEnd+" size="+experiments.size() 
}

// Setup RA files if requested...
RAUtils ra;
if (options.raRoot){
  err.println "Creating new ra files: "+options.raRoot;
  ra = new RAUtils(options.raRoot);
  ra.writeFeatureSelection(experimentStart);   // currently, always say None
}


// Write out heading for the stats output... there are a lot, so heading needed. 
// don't want to print it when running as cluster job, because we'll be concatenating 
// output from lots of jobs, heading would appear hundreds of times in file. 
if (options.writeHeading){
  summaryHeading = WekaExplorer.getFormattedSummaryHeading()
  out<< "${summaryHeading},classifier, attributeEval,attributeSearch,numAttributes,classAttribute\n"  
  System.exit(1);
}


/***********************************************************************************
*                               MAIN PROCESSING 
*/ 

// Read data and clinical files as a set of Weka Instances
Instances data = WekaExplorer.readNumericFromTable(options.data)
Instances clinicalData = WekaExplorer.readNumericFromTable(options.clinical)

// For each experiment...
def tid,sid,classID;
experiments[experimentStart..experimentEnd].eachWithIndex{experiment,idx-> 

// Trying to improve error reporting on cluster...
try{
      
 // err.println "\nEXP.toString: "+experiment.toString()
  
  pipeline = new WekaExplorer(experiment.classAttribute)
  
  // Remove all clinical attributes except for the current class...
  selectedAttribute = []
  selectedAttribute.add(experiment.classAttribute)
  singleClinicalAttribute = pipeline.removeAttributesNotSelected(clinicalData,selectedAttribute)  

  // Merge data and clinical files (i.e. instances contained in both, omitting rest)
  merged = IU.mergeNamedInstances(data,singleClinicalAttribute)

  // Remove any attributes that don't vary at all..
  merged = pipeline.removeUselessAttributes(merged)
    
  // For many class-attributes, negative values are invalid.  
  if (!params.allowNegativeClass){    
    merged = pipeline.removeInstancesWithNegativeClassValues(merged)
  }

  // Remove any instances whose class value is missingValue(). 
  // KJD why not:  newData.deleteWithMissingClass();
  merged = pipeline.removeInstancesWithMissingClassValues(merged) 

  // Discretize class (needs to be an experiment list parameter, not global option)
  if (params.useMedianDiscretization){
    merged = pipeline.classToMedian(merged,"low","high")        
    //err.println "Not yet implemented:  median discretization."
    //System.exit(1)
  }else if (params.useUpperLowerQuartile){
    merged = pipeline.classToNominalTopBottomQuartile(merged,"low","high")        
  }else if (params.discretizeCutoffs){
    lowerBound = params.discretizeCutoffs.low
    upperBound = params.discretizeCutoffs.high
    merged = pipeline.classToNominalFromCutoffs(merged,lowerBound,upperBound,"low","high")
  }

  // If an arffFile output name was given, just save this processed file for 
  // use in Weka GUI and exit.  Arff files aren't a core part of the pipeline in 
  // any way, but sometimes it's useful to explore the pre-processed data in 
  // the GUI
  if (params.arffFile){
    ArffSaver saver = new ArffSaver();
    saver.setInstances(merged);
    err.print "\nSaving ${params.arffFile} for manual processing..."
    saver.setFile(new File(params.arffFile))
    saver.writeBatch();  
    err.println "done. Exiting."
    System.exit(0); // Exit happily. 
  }

  // Check to see if we can do this experiment at all...
  // KJD TODO:  Should also check to see if there is more than one class left...
  if (merged.numInstances() < params.cvFolds){
    err.println "Experiment skipped.  ${merged.numInstances()} instances < ${params.cvFolds} cross-validation folds."
    return; // Can't do the experiment. 
  } 
    
  // KJD:TODO   
  // If it's just a feature selection-only experiment, do that experiment,
  if (experiment.classifier == null){
    AttributeSelection attsel = new AttributeSelection(); 

    // Remove instance ID which is a string... 
    filter = new RemoveType();
    filter.setInputFormat(merged);
    filtered = Filter.useFilter(merged, filter);

    // Only Ranker (? right??) allows an overt numAttributes cutoff. 
    // At least, BestFirst search does not. 
    search = experiment.attributeSearch
    if (search.class == Ranker){
      search.setNumToSelect(experiment.numAttributes);
    }    
    
    attsel.setEvaluator(experiment.attributeEval);
    attsel.setSearch(search);
    attsel.SelectAttributes(filtered);     
          
    WekaPipelineOutput.appendAttributeSelectionSummaryLine(merged,out,
                                         merged.numInstances(),
                                         experiment,attsel,
                                         options.maxFeaturesOut as Integer)                                             
    return;
  }
  
  // otherwise, wrap classifier in feature selection evaluator...                  
  def asClassifier
  if (experiment.attributeEval == null) asClassifier = pipeline.classifierFromSpec(classifierSpec)
  else{
    
    // Wrap classifier in an AttributeSelectedClassifier2 (modified from weka to 
    // provide API to expose the actual attributes selected). 
    asClassifier = new AttributeSelectedClassifier2();
    
    search = experiment.attributeSearch
    // Only Ranker (? right??) allows an overt numAttributes cutoff. 
    // At least, BestFirst search does not. 
    if (search.class == Ranker){
      search.setNumToSelect(experiment.numAttributes);
    }
    
    asClassifier.setClassifier(experiment.classifier);
    asClassifier.setEvaluator(experiment.attributeEval);
    asClassifier.setSearch(search);                
  }  
       
  bugNumAttr = merged.numAttributes()
  (0..<bugNumAttr).each{
    attr = merged.attribute(it);
    if (attr.isNominal()){
      println "Nominal attribute: ${attr.name()}";
    }
  }
   
  // Perform cross-validation of the model..
  err.print "Instances: ${merged.numInstances()} Attributes: ${experiment.numAttributes}. Running cross validation (${params.cvFolds} folds)..."
    
  cvu = new CVUtils()
  results = cvu.crossValidateModel(asClassifier,merged,params.cvFolds,new Random(params.cvSeed))
  eval = cvu.eval
    
  if (options.raRoot){
    // Record basic info in RA files... ra functions handle detecting duplicate classifiers and subgroups...
    // at least within a job... between jobs, a post-filter will be needed. 
    jobIdx = idx+experimentStart;
    classIDTemp = ra.writeClassifier(experiment.classifierStr,jobIdx); 
    //  def classID1,tid1,sid1;
    if (classIDTemp != null) classID = classIDTemp;  
    err.println "classID: $classID"
  
    (tid1,sid1) = ra.writeSubgroup(merged,params,jobIdx);
    if (tid1 != null){
      tid = tid1;
      sid = sid1;
    }   
    ra.writeResults(results,classID,tid,sid,jobIdx)
  }
    
  err.println "done. ${experiment.classifierStr} #att: ${experiment.numAttributes} AUC=${eval.weightedAreaUnderROC()}"
  
  WekaPipelineOutput.appendSummaryLine(merged,out,merged.numInstances(),
                                      experiment,eval,
                                      options.maxFeaturesOut as Integer)
                                                                                                                
  // Save an ROC directory if requested...
  if (params.rocDir) WekaPipelineOutput.saveROC(eval,params.rocDir,className,numFeatures)         
 
  }catch(Exception e){  
    // Save something meaningful so we know what happened, then move on...
    jobIdx = idx+experimentStart;
    errfile = "${options.output}.err"
    errout = new File(errfile)
    errmsg = e.toString()
    err.println "ERROR on experiment #$jobIdx"
    errout.append("ERROR on experiment #$jobIdx" as String)
    errout.append("\nExperiment: ")
    errout.append(experiment.toString())
    errout.append("\n")
    err.println "$errmsg"
    errout.append(errmsg)
    errout.append("\n\n")
  }  

} // classifiers

// Example: 
// scripts/paradigmClassifierEval -d dataDec9/breast/chin_naderi_pathwayvalues.txt -i dataDec9/breast/chinNaderi_clinical.txt -c Survival -D 9,11 > test/summary.txt

