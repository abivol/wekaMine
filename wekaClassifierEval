#!/usr/bin/env groovy 

import durbin.weka.*
import durbin.weka.InstanceUtils as IU
import durbin.util.*
import durbin.paradigm.*

import weka.core.*
import weka.core.converters.ArffSaver
import weka.filters.*
import weka.filters.unsupervised.attribute.Remove
import weka.filters.unsupervised.instance.RemoveWithValues

import weka.classifiers.*
import weka.classifiers.evaluation.*
import weka.classifiers.Evaluation
import weka.classifiers.functions.supportVector.PolyKernel

import weka.classifiers.meta.*
import weka.attributeSelection.*
import weka.attributeSelection.InfoGainAttributeEval

import weka.classifiers.meta.GridSearch;

// Alias sugar. 
err = System.err

// Add some sugar methods to weka.Instances with Groovy metaclass. 
WekaAdditions.enable()     

parser = new Parser(description: '''
 paradigmClassifierEval takes a data file in attributes (rows) x samples (cols) format, 
 and a clinical file, also in attributes (rows) x samples (cols) format, and evaluate 
 the given Weka classifier predicting a specified clinical attribute.  Classification is 
 performed on the samples that occur in both the data and clinical file (intersection). 
 stdout is a list of summary lines in CSV format. Instances with missing class values are removed.   
 Attributes that do not vary at all are removed.  Instances with negative class values are 
 optionally removed.  
 
 Script written to process output of PARADIGM.  Written by: James Durbin (kdurbin@ucsc.edu)
 
 Example:
 
 paradigmClassifierEval -d chin_naderi_pathwayvalues.txt -i chinNaderi_clinical.txt -c Survival -D 9,11 > summary.txt 
 
''');

parser.with{
  required 'd','data', [description: 'Data file in attribute (row) by samples (col) format.']
  required 'i','clinical', [description: 'Clinical file in attribute (row) by samples (col) format.']
  
  optional 'c','class', [description: 'Clinical feature to use as class. MUST specify either -c, -C, or -a']
  optional 'C','classFile',[description: 'File containing list of clinical attributes to classify on. MUST specify either -c, -C, or -a']  

  optional 'm','classifierFile', [description: 'File name of weka classifiers to try.  Default is SMO linear kernel.']
  optional 's','classifierSpec', [description: 'Single classifier specification string.']
  optional 'f','featureSelectionFile', [description: 'File name of weka attribute filters to try. Default is Fisher LD']
  
  optional 'D','discretizeCutoffs',[description: 'High/Low cutoff values for discretization in comma separated string: low,high.']
  
  optional 'n',"numAttributes",[default: '10,200,20', 
                description: 'Range of attributes specified as comma separated string: start,stop,step']
  
  optional 'F','cvFolds',[default:5,description: 'Number of cross validation folds.',validate:{it as int}]
  optional 'S','cvSeed',[default:1,description: 'Cross-validation random seed.',validate:{it as int}]
  
  optional 'r','rocDir',[description: 'Directory to save ROC plots in Arff format.']
  
  optional "p",'prefilter',[description: 'Specify an attribute to prefilter on and range of values (e.g. Survival,<8). Only < and > supported.']
  
//  optional 'p','featuresPlot',[description: 'Save plots of AUC vs #Features.']

  optional 'P','arffFile',[description: 'Save processed instances to arffFile for use in Weka GUI. Disables other processing.']
    
  flag 'N','allowNegativeClass',[default:false,description: 'If set, negative class values are allowed.']
  flag 'a','useAllClinicalAttributes',[default:false,description: 'If set, will classify on each clinical attribute in turn.']
  flag 'A','noAttributeSelection',[default:false,description: 'If set, attribute selection is disabled.']
  flag 'M','useMedianDiscretization',[default:false,description: 'If set, discretization will divide high/low on median.']
  flag 'U','useUpperLowerQuartile',[default:false,description: 'If set, discretization will divide high/low on upper/lower quartile.']
  flag 'h','help',[default:false,description: 'Print script help.']
}

// Validate attributes that one of a set are required...
parser.validate{parameters->
  count = 0
  if (parameters['c']) count++
  if (parameters['C']) count++
  if (parameters['a']) count++
  if (count != 1) throw new Exception("Must specify exactly one of: -c, -C, or -a.")
  
  count = 0;
  if (parameters['M']) count++
  if (parameters['U']) count++
  if (parameters['D']) count++
  // If we're going to save to an arff file, can skip discretization...
  if ((count != 1) && (!parameters['P'])) throw new Exception("Must specify exactly one of -M, -U, or -D.")
}

def params
try{
  params = parser.parse(args)
}catch(Exception e){
  System.err << parser.usage
  System.exit(1)
}

/***********************************************************************************
*                                      SETUP
*/ 

// Extract the bounds for feature size trials...
minFeatures = params.numAttributes.split(",")[0] as int
maxFeatures = params.numAttributes.split(",")[1] as int
featureStep = params.numAttributes.split(",")[2] as int 

// Set/load classifier descriptions...
classifiers = []
if (params.classifierFile) new File(params.classifierFile).eachLine{if (!it.startsWith('#')) classifiers << it}
else if (params.classifierSpec) classifiers << params.classifierSpec
else classifiers << "weka.classifiers.functions.SMO -C 1.0 -L 0.0010 -P 1.0E-12 -N 0 -V -1 -W 1 -M"

err.println "*******Classifiers: "+classifiers


// Set/load attribute selection evaluator...
evals = []
if (params.featureSelectionFile) new File(params.featureSelectionFile).eachLine{if (!it.startsWith('#')) evals << it}
else evals << "durbin.weka.FisherLDEval"

// Use all clinical attributes, or one specific clinical attribute, or a list from a file. 
def clinicalAttributes = []
if (params.useAllClinicalAttributes){
  clinicalData.attributeNames().each{
    if (it != 'ID') clinicalAttributes << it  // Skip the ID attribute. 
  }
}else{
  if (params.class) clinicalAttributes << params.class
  else{
    new File(params.classFile).eachLine{clinicalAttributes << it}
  }
}

/***********************************************************************************
*                               MAIN PROCESSING 
*/ 

// Write out heading for the stats output... there are a lot, so heading needed. 
summaryHeading = ParadigmPipeline.getFormattedSummaryHeading()
println "className,${summaryHeading},numAttributes,classifier,attributeSelection"

// Read data and clinical files as a set of Weka Instances
Instances data = ParadigmPipeline.readFromTable(params.data)
Instances clinicalData = ParadigmPipeline.readFromTable(params.clinical)

// For each classifier...
classifiers.each{classifierSpec-> 
  // For each attribute filter...
  evals.each{evalSpec->
    // For each clinical attribute to classify on...
    clinicalAttributes.each{className->
      
      pipeline = new ParadigmPipeline(className)
  
      println "Classifier spec: $classifierSpec"
      classifier = pipeline.classifierFromSpec(classifierSpec)
      attrEval = pipeline.evalFromSpec(evalSpec)

      // Remove all clinical attributes except for the current class...
      selectedAttribute = []
      selectedAttribute.add(className)
      singleClinicalAttribute = pipeline.removeAttributesNotSelected(clinicalData,selectedAttribute)  

      // Merge data and clinical files (i.e. instances contained in both, omitting rest)
      merged = IU.mergeNamedInstances(data,singleClinicalAttribute)

      // Remove any attributes that don't vary at all..
      merged = pipeline.removeUselessAttributes(merged)
    
      // For many class-attributes, negative values are invalid.  
      if (!params.allowNegativeClass){    
        merged = pipeline.removeInstancesWithNegativeClassValues(merged)
      }

      // Remove any instances whose class value is missingValue(). 
      // KJD why not:  newData.deleteWithMissingClass();
      merged = pipeline.removeInstancesWithMissingClassValues(merged) 

      // Discretize class 
      if (params.useMedianDiscretization){
        //merged = pipeline.classToMedian(merged,"low","high")        
        err.println "Not yet implemented:  median discretization."
        System.exit(1)
      }else if (params.useUpperLowerQuartile){
        merged = pipeline.classToNominalTopBottomQuartile(merged,"low","high")        
      }else if (params.discretizeCutoffs){
        lowerBound = params.discretizeCutoffs.split(",")[0] as double
        upperBound = params.discretizeCutoffs.split(",")[1] as double
        merged = pipeline.classToNominalFromCutoffs(merged,lowerBound,upperBound,"low","high")
      }

      // If an arffFile output name was given, just save this processed file for 
      // use in Weka GUI and exit. 
      if (params.arffFile){
        ArffSaver saver = new ArffSaver();
        saver.setInstances(merged);
        err.print "\nSaving ${params.arffFile} for manual processing..."
        saver.setFile(new File(params.arffFile))
        saver.writeBatch();  
        err.println "done."
        System.exit(0); // Exit happily. 
      }
      
      // KJD... should do this with filteredClassifier...
      merged = pipeline.removeID(merged)    
      
      // Check to see if we can do this experiment at all...
      if (merged.numInstances() < params.cvFolds) {
        err.println "Experiment skipped.  ${merged.numInstances} instances < ${params.cvFolds} cross-validation folds."
        return; // Can't do the experiment. 
      } 
                
      /****************************************************
      *   For each feature cutoff 
      */
      minFeatures.step(maxFeatures,featureStep){numFeatures->  
        def asClassifier
        if (params.noAttributeSelection) asClassifier = classifierFromSpec(classifierSpec)
        else{
          // Wrap classifier in an AttributeSelectedClassifier. 
          asClassifier = new AttributeSelectedClassifier();
          search = new Ranker();
          search.setNumToSelect(numFeatures)
          asClassifier.setClassifier(classifier);
          asClassifier.setEvaluator(attrEval);
          asClassifier.setSearch(search);                
        }  
    
        // KJD TODO:  Need to Skip cases where the class doesn't vary!!!
   
        // Perform cross-validation of the model..
        err.print "Instances: ${merged.numInstances()} Attributes: $numFeatures. Running cross validation (${params.cvFolds} folds)..."
        eval = new Evaluation(merged)
        predictions = new StringBuffer()
        eval.crossValidateModel(asClassifier,merged,params.cvFolds,
          new Random(params.cvSeed),predictions,
          new Range("first,last"),false)
        err.println "done. $className #att: $numFeatures AUC=${eval.weightedAreaUnderROC()}"

        //pipeline.parsePredictions(predictions)

        // Append a summary line to a file. 
        summaryLine = pipeline.getFormattedEvaluationSummary(merged.numInstances(),eval)
        println className+","+summaryLine+",$numFeatures,"+classifierSpec+","+evalSpec      

        if (params.rocDir){
          // Save the ROC values for later plotting. 
          ThresholdCurve tc = new ThresholdCurve();
          Instances curve = tc.getCurve(eval.predictions());
    
          ArffSaver saver = new ArffSaver();
          saver.setInstances(curve);
          curveFile = new File("${params.rocDir}${className}.${numFeatures}.arff".toString())
          saver.setFile(curveFile);
          saver.writeBatch();        
        }
      } // #features
    } // clinicalAttributes (className)
  }  // attribute selection
} // classifiers



// Example: 
// scripts/paradigmClassifierEval -d dataDec9/breast/chin_naderi_pathwayvalues.txt -i dataDec9/breast/chinNaderi_clinical.txt -c Survival -D 9,11 > test/summary.txt

