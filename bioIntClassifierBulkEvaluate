#!/usr/bin/env groovy 

import durbin.weka.*
import durbin.weka.InstanceUtils as IU
import durbin.util.*

import weka.core.*
import weka.core.converters.ArffSaver
import weka.filters.*
import weka.filters.unsupervised.attribute.Remove
import weka.filters.unsupervised.instance.RemoveWithValues

import weka.classifiers.*
import weka.classifiers.evaluation.*
import weka.classifiers.Evaluation
import weka.classifiers.functions.supportVector.PolyKernel

import weka.classifiers.meta.*
import weka.attributeSelection.*
import weka.attributeSelection.InfoGainAttributeEval

import weka.classifiers.meta.GridSearch;

// Alias sugar. 
err = System.err

// Add some sugar methods to weka.Instances with Groovy metaclass. 
WekaAdditions.enable()     

/***********************************************************************************
*                            Define Command Line Options
*/
parser = new Parser(description: '''
 bioIntClassifierEval reads data and experiment specification from the bioIntegrator database, 
 performs the experiments and concatenates the results to a file.  Cluster jobs can be given 
 non-overlapping ranges of experiments to run with the -r option.  
 
Written by: James Durbin (kdurbin@ucsc.edu)

''');

parser.with{
  required 'd','database',[description: 'Name of database (e.g. bioInt, bioIntTCGAOV)']
  required 'm','machine',[description: 'Name of machine db is on (e.g. tcga1.cse.ucsc.edu)']
  required 'c','configID',[description: 'Id of the experiment config file for this experiment.']
  required 'o','output',[description: 'File where output should go.']
  
  //optional 'P','arffFile',[description: 'Save processed instances to arffFile for use in Weka GUI. Disables other processing.']
  
  optional 'h','hgconf',[default:'.hg.conf.tcga1', description: "Alternate conf file to .hg.conf.tcga1 for database user/password."]
  optional 'r','experimentRange',[description: 'Range of experiments to run. e.g. -r 54,67']
  optional 'p','portmapping',[description: 'Port mapping to use for ssh tunneling.  e.g. -p 3307,3306']
  flag 'h','help',[default:false,description: 'Print script help.']
}

def options
try{
  options = parser.parse(args)
}catch(Exception e){
  System.err << parser.usage
  System.exit(1)
}


/***********************************************************************************
*                                 Port Mapping
*/

// Since bioInt mysql is behind a firewall, need to do ssh port forwarding
// to make a local port here look like a local port there... 
// SSHPortForwarder will look at $HOME/.ssh/id_rsa for the private key 
// for the key-pair.  
pf = new SSHPortForwarder()
pf.connect(user="james",host=options.machine,
           rhost="127.0.0.1",lport=3307,rport=3306)

/***********************************************************************************
*                                Connect to DB
*/
err.print "Connecting to db..."
bioint = new BioIntLoader(hgconf,3307,options.database)
err.println "done."

/***********************************************************************************
*                                     SETUP
*/ 
def slurper = new WekaExplorerConfigSlurper()
def cfg = slurper.parseDB(bioint.db,options.configID)

params = cfg.params // cfg includes all sections.  make alias for section we want. 

experiments = []
if (options.experimentList){
  new File(options.experimentList).eachLine{line->
    if (!line.startsWith('//')){      
      experiments << new ExperimentSpec(line);
    }
  }
}else{
  slurper.getExpansion('experiments').each{experiment->
    experiments << new ExperimentSpec(experiment);
  }
}

// Set the 
experimentStart = 0
experimentEnd = experiments.size()-1
if (options.experimentRange){
 experimentStart = options.experimentRange.split(",")[0] as int
 experimentEnd = options.experimentRange.split(",")[1] as int
 
 err.println "eStart:"+experimentStart+" eEnd: "+experimentEnd+" size="+experiments.size() 
}

/***********************************************************************************
*                               MAIN PROCESSING 
*/ 

// Buffering writes...
new File(options.output).withWriter{out->

// Write out heading for the stats output... there are a lot, so heading needed. 
summaryHeading = ParadigmPipeline.getFormattedSummaryHeading()

// don't want to print it because we'll be concatenating output from lots of jobs, heading would 
// appear hundreds of times in file. (should make it an option.)
out<< "${summaryHeading},classifier, attributeEval,attributeSearch,numAttributes,classAttribute\n"

//Instances data = ParadigmPipeline.readFromTable(options.data)
Instances data = bioint.getInstances(table)
-->clinicalData = bioint.getClinicalData(table)

//Instances clinicalData = ParadigmPipeline.readFromTable(options.clinical)

// For each experiment...
experiments[experimentStart..experimentEnd].each{experiment-> 
  
  err.println experiment.toString()
  
  pipeline = new ParadigmPipeline(experiment.classAttribute)
  
   // will assume there is already a spot for it...
   // data modified in-place.
-->pipeline.addClinicalAttribute(data,clinicalData,experiment.classAttribute)
  
  // Remove any attributes that don't vary at all..
--> note data plugged in here...  merged = pipeline.removeUselessAttributes(data)
    
  // For many class-attributes, negative values are invalid.  
  if (!params.allowNegativeClass){    
    merged = pipeline.removeInstancesWithNegativeClassValues(merged)
  }

  // Remove any instances whose class value is missingValue(). 
  // KJD why not:  newData.deleteWithMissingClass();
  merged = pipeline.removeInstancesWithMissingClassValues(merged) 

  // Discretize class 
  if (params.useMedianDiscretization){
    //merged = pipeline.classToMedian(merged,"low","high")        
    err.println "Not yet implemented:  median discretization."
    System.exit(1)
  }else if (params.useUpperLowerQuartile){
    merged = pipeline.classToNominalTopBottomQuartile(merged,"low","high")        
  }else if (params.discretizeCutoffs){
    lowerBound = params.discretizeCutoffs.low
    upperBound = params.discretizeCutoffs.high
    merged = pipeline.classToNominalFromCutoffs(merged,lowerBound,upperBound,"low","high")
  }

  // If an arffFile output name was given, just save this processed file for 
  // use in Weka GUI and exit. 
  if (params.arffFile){
    ArffSaver saver = new ArffSaver();
    saver.setInstances(merged);
    err.print "\nSaving ${params.arffFile} for manual processing..."
    saver.setFile(new File(params.arffFile))
    saver.writeBatch();  
    err.println "done. Exiting."
    System.exit(0); // Exit happily. 
  }
      
  // KJD... should do this with filteredClassifier...
  merged = pipeline.removeID(merged)        
      
  // Check to see if we can do this experiment at all...
  if (merged.numInstances() < params.cvFolds) {
    err.println "Experiment skipped.  ${merged.numInstances} instances < ${params.cvFolds} cross-validation folds."
    return; // Can't do the experiment. 
  }
                
  def asClassifier
  if (params.noAttributeSelection) asClassifier = classifierFromSpec(classifierSpec)
  else{
    // Wrap classifier in an AttributeSelectedClassifier. 
    asClassifier = new AttributeSelectedClassifier();
    
    search = experiment.attributeSearch
    // Only Ranker (? right??) allows an overt numAttributes cutoff. 
    // At least, BestFirst search does not. 
    if (search.class == Ranker){
      search.setNumToSelect(experiment.numAttributes);
    }
    
    asClassifier.setClassifier(experiment.classifier);
    asClassifier.setEvaluator(experiment.attributeEval);
    asClassifier.setSearch(search);                
  }  
    
  // KJD TODO:  Need to Skip cases where the class doesn't vary!!!
   
  // Perform cross-validation of the model..
  err.print "Instances: ${merged.numInstances()} Attributes: ${experiment.numAttributes}. Running cross validation (${params.cvFolds} folds)..."
  eval = new Evaluation(merged)
  eval.crossValidateModel(asClassifier,merged,params.cvFolds,new Random(params.cvSeed))
  err.println "done. ${experiment.classifierStr} #att: ${experiment.numAttributes} AUC=${eval.weightedAreaUnderROC()}"

  // Append a summary line to a file. 
  summaryLine = pipeline.getFormattedEvaluationSummary(merged.numInstances(),eval)
  out << "$summaryLine,${experiment.classifierStr},${experiment.attrEvalStr},${experiment.attrSearchStr},${experiment.numAttributes},${experiment.classAttribute}\n"

  if (params.rocDir){
    // Save the ROC values for later plotting. 
    ThresholdCurve tc = new ThresholdCurve();
    Instances curve = tc.getCurve(eval.predictions());
    
    ArffSaver saver = new ArffSaver();
    saver.setInstances(curve);
    curveFile = new File("${params.rocDir}${className}.${numFeatures}.arff".toString())
    saver.setFile(curveFile);
    saver.writeBatch();        
  }
} // classifiers

}

// If you don't put this in, script will never exit...
pf.disconnect()

// Example: 
// scripts/paradigmClassifierEval -d dataDec9/breast/chin_naderi_pathwayvalues.txt -i dataDec9/breast/chinNaderi_clinical.txt -c Survival -D 9,11 > test/summary.txt

